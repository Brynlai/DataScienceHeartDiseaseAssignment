{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brynlai/DataScienceHeartDiseaseAssignment/blob/Bryan/AssignmentCompletedV5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. Data Understanding\n",
        "1. Data Collection\n",
        "2. Data Description\n",
        "3. Data Exploration"
      ],
      "metadata": {
        "id": "GMwkE05qR-ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B.1. Data Collection"
      ],
      "metadata": {
        "id": "9wvkuMLzTO2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B. Data Understanding - 1. Data Collection\n",
        "# @title\n",
        "!pip install ucimlrepo\n",
        "!pip install pandas matplotlib seaborn scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Fetch Heart Disease dataset from UCI ML Repository\n",
        "heart_disease_bunch = fetch_ucirepo(id=45)\n",
        "\n",
        "# Load into DataFrame\n",
        "heart_disease = pd.DataFrame(data=heart_disease_bunch.data.features,\n",
        "                             columns=heart_disease_bunch.data.feature_names,\n",
        "                             index=heart_disease_bunch.data.ids)\n",
        "heart_disease = pd.concat([heart_disease, heart_disease_bunch.data.targets], axis=1)"
      ],
      "metadata": {
        "id": "rQJYLjk7o-mg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B.2. Data Description"
      ],
      "metadata": {
        "id": "EKwa1c9YPu67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B. Data Understanding: - 2. Data Description\n",
        "def print_col(col):\n",
        "  # Get fequency of each result in groups\n",
        "  group_sizes = df.groupby(col).size()\n",
        "  print(f\"Column: {'col'}\")\n",
        "  print(group_sizes)\n",
        "  print()\n",
        "\n",
        "def print_group_sizes(df):\n",
        "    for column in df.columns:\n",
        "        # Get fequency of each result in groups\n",
        "        print_col(column)"
      ],
      "metadata": {
        "id": "gC6_zKXU8MPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show column names, null value count and data type\n",
        "print(df.info())\n",
        "\n",
        "# Copy hear_disease to df.\n",
        "df = heart_disease\n",
        "\n",
        "# Rename num to target.\n",
        "df = df.rename(columns={'num': 'target'})\n",
        "\n",
        "# Rename Columns to make them more readable\n",
        "column_names = {\n",
        "    \"age\": \"Age\",\n",
        "    \"sex\": \"Gender\",\n",
        "    \"cp\": \"ChestPainType\",\n",
        "    \"trestbps\": \"RestingBP\",\n",
        "    \"fbs\": \"FastBloodSugar\",\n",
        "    \"restecg\": \"RestingECG\",\n",
        "    \"exang\": \"ExerciseAngina\",\n",
        "    \"slope\": \"ExerciseSlope\",\n",
        "    \"ca\": \"MajorVessels\",\n",
        "    \"thal\": \"ThalliumStress\",\n",
        "    \"target\": \"HeartDisease\",\n",
        "    \"chol\": \"SerumCholesterol\",\n",
        "    \"thalach\": \"MaxHeartRate\",\n",
        "    \"oldpeak\": \"OldPeak\"\n",
        "}\n",
        "df.rename(columns=column_names, inplace=True)\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "DzHjMHGZ7tMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B.3 Data Exploration"
      ],
      "metadata": {
        "id": "mt0Em00z9sOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show data types and null\n",
        "print(df.info())\n",
        "\n",
        "# Summary Statistics\n",
        "summary_stats = df.describe()\n",
        "print(summary_stats)"
      ],
      "metadata": {
        "id": "QgcbqwYXCEAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution Plots of the columns values\n",
        "for column in df.columns:\n",
        "    sns.histplot(df[column], kde=True, bins=20)\n",
        "    plt.title(f'Distribution of {column}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KCEjpFJFBtaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a pair plot with customization\n",
        "# sns.pairplot(df, vars=df.columns, diag_kind='kde', markers='o')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "8cDrD6yT9vlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. Data Preperation\n",
        "1. Data Cleaning\n",
        "2. Data transformation"
      ],
      "metadata": {
        "id": "xQKayQXHTFQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C.1. Data Cleaning"
      ],
      "metadata": {
        "id": "4yz1Rg_QT51F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Data Preperation - 1. Data Cleaning\n",
        "# Check for any duplicate observation\n",
        "duplicate_rows = df.duplicated()\n",
        "print(\"Number of duplicate rows before:\", duplicate_rows.sum())\n",
        "\n",
        "# Remove duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Check for duplicate rows again\n",
        "duplicate_rows = df.duplicated()\n",
        "print(\"Number of duplicate rows after:\", duplicate_rows.sum())"
      ],
      "metadata": {
        "id": "AtzvYm_rSZOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Data Preperation - 1. Data Cleaning\n",
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Replace missing values with median of each column, only if the column is numerical\n",
        "for column in df.columns:\n",
        "    if df[column].dtype in [np.int64, np.float64]:  # Check if the column is numerical\n",
        "        df[column] = df[column].fillna(df[column].median())\n",
        "\n",
        "# Check if there are any missing values left\n",
        "missing_values_after = df.isnull().sum()\n",
        "print(\"Missing values after replacing with medians:\")\n",
        "print(missing_values_after)"
      ],
      "metadata": {
        "id": "rTxsFBhESeN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Data Preperation - 1. Data Cleaning\n",
        "def remove_outliers(df, columns, threshold=3):\n",
        "    outliers = {}\n",
        "    for column in columns:\n",
        "        mean = df[column].mean()\n",
        "        std = df[column].std()\n",
        "        z_scores = np.abs((df[column] - mean) / std)\n",
        "        outlier_indices = z_scores >= threshold\n",
        "        outliers[column] = df[column][outlier_indices].tolist()\n",
        "        df = df[~outlier_indices]\n",
        "    return df, outliers\n",
        "\n",
        "# Specify columns to check for outliers\n",
        "columns_to_check = ['RestingBP', 'SerumCholesterol', 'MaxHeartRate', 'OldPeak']\n",
        "\n",
        "# Remove outliers from the DataFrame and print the outliers\n",
        "df_cleaned, outliers = remove_outliers(df, columns_to_check)\n",
        "for column, outlier_list in outliers.items():\n",
        "    print(f\"Outliers in {column}: {outlier_list}\")\n",
        "\n",
        "# Create distribution plots before and after removing outliers\n",
        "fig, axes = plt.subplots(nrows=len(columns_to_check), ncols=2, figsize=(12, 6*len(columns_to_check)))\n",
        "for i, column in enumerate(columns_to_check):\n",
        "    sns.histplot(df[column], ax=axes[i, 0], kde=True, bins=20)\n",
        "    axes[i, 0].set_title(f'Distribution of {column} Before Removing Outliers')\n",
        "    sns.histplot(df_cleaned[column], ax=axes[i, 1], kde=True, bins=20)\n",
        "    axes[i, 1].set_title(f'Distribution of {column} After Removing Outliers')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MMqU1E4tSjMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C.2. Data Transformation"
      ],
      "metadata": {
        "id": "49siaE_LUAdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before\n",
        "print_col('HeartDisease')\n",
        "\n",
        "# Change anything above 0 into 1\n",
        "df['HeartDisease'] = df['HeartDisease'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# After\n",
        "print_col('HeartDisease')"
      ],
      "metadata": {
        "id": "iQ3DxT5G_VeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Data Preperation - 1. Data Transformation\n",
        "columns_to_binned = ['Age', 'RestingBP', 'SerumCholesterol', 'MaxHeartRate', 'OldPeak']\n",
        "\n",
        "# # Before:\n",
        "# print(\"\\n\\nBefore Binning: -----------\")\n",
        "# for column in columns_to_binned:\n",
        "#     print_col(column)\n",
        "\n",
        "# Visualize before binning\n",
        "fig, axes = plt.subplots(nrows=len(columns_to_binned), ncols=1, figsize=(8, 6*len(columns_to_binned)))\n",
        "for i, column in enumerate(columns_to_binned):\n",
        "    sns.histplot(df[column], ax=axes[i], kde=True, bins=20)\n",
        "    axes[i].set_title(f'Distribution of {column} Before Binning')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Bins for age\n",
        "age_bins = [10, 20, 30, 40, 50, 60, np.inf]\n",
        "age_labels = [1, 2, 3, 4, 5, 6]  # Assign numerical labels\n",
        "df['Age_binned'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, include_lowest=True)\n",
        "\n",
        "# Bins for resting blood pressure\n",
        "trestbps_bins = [0, 100, 120, 140, 160, np.inf]\n",
        "trestbps_labels = [1, 2, 3, 4, 5]  # Assign numerical labels\n",
        "df['RestingBP_binned'] = pd.cut(df['RestingBP'], bins=trestbps_bins, labels=trestbps_labels, include_lowest=True)\n",
        "\n",
        "# Bins for serum cholesterol\n",
        "chol_bins = [0, 160, 200, 240, 280, np.inf]\n",
        "chol_labels = [1, 2, 3, 4, 5]  # Assign numerical labels\n",
        "df['SerumCholesterol_binned'] = pd.cut(df['SerumCholesterol'], bins=chol_bins, labels=chol_labels, include_lowest=True)\n",
        "\n",
        "# Bins for maximum heart rate achieved\n",
        "thalach_bins = [0, 90, 120, 150, 180, np.inf]\n",
        "thalach_labels = [1, 2, 3, 4, 5]  # Assign numerical labels\n",
        "df['MaxHeartRate_binned'] = pd.cut(df['MaxHeartRate'], bins=thalach_bins, labels=thalach_labels, include_lowest=True)\n",
        "\n",
        "# Bins for ST depression induced by exercise\n",
        "oldpeak_bins = [0, 0.5, 1.5, 2.5, 3.5, np.inf]\n",
        "oldpeak_labels = [1, 2, 3, 4, 5]  # Assign numerical labels\n",
        "df['OldPeak_binned'] = pd.cut(df['OldPeak'], bins=oldpeak_bins, labels=oldpeak_labels, include_lowest=True)\n",
        "\n",
        "# Drop original numerical columns\n",
        "df = df.drop(['Age', 'RestingBP', 'SerumCholesterol', 'MaxHeartRate', 'OldPeak'], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "columns_binned = ['Age_binned', 'RestingBP_binned', 'SerumCholesterol_binned', 'MaxHeartRate_binned', 'OldPeak_binned']\n",
        "# After:\n",
        "# print(\"\\n\\nAfter Binning: -----------\")\n",
        "# for column in columns_binned:\n",
        "#     print_col(column)\n",
        "\n",
        "# Visualize after binning\n",
        "fig, axes = plt.subplots(nrows=len(columns_binned), ncols=1, figsize=(8, 6*len(columns_binned)))\n",
        "for i, column in enumerate(columns_binned):\n",
        "    sns.histplot(df[column], ax=axes[i], kde=False, bins=None)\n",
        "    axes[i].set_title(f'Distribution of {column} After Binning')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "lmpOp7ws7H0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax = sns.heatmap(corr_matrix,\n",
        "                 annot=True,\n",
        "                 linewidths=0.5,\n",
        "                 fmt=\".2f\",\n",
        "                 cmap=\"YlGnBu\");\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fk5GPMa93feu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In the correlation matrix, these attributes were +/- 0.25.\n",
        "relevant_features = ['HeartDisease','Gender', 'ChestPainType', 'ExerciseAngina', 'ExerciseSlope',\n",
        "                     'MajorVessels', 'ThalliumStress', 'MaxHeartRate_binned', 'OldPeak_binned']\n",
        "\n",
        "# Clean and Simple Histograms\n",
        "for column in relevant_features:\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.histplot(df[column], ax=ax, kde=True, bins=20)\n",
        "    ax.set_title(f'Distribution of {column}')\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel('Frequency')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FLOUsazzJjbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D. Modelling\n",
        "1. Scale\n",
        "2. Set seed for reproducibility\n",
        "3. Split data\n",
        "4. Train Data\n",
        "5. Print Results"
      ],
      "metadata": {
        "id": "jJseLVYdUnT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the relevant features\n",
        "# In the correlation matrix, these attributes were +/- 0.25.\n",
        "relevant_features = ['Gender', 'ChestPainType', 'ExerciseAngina', 'ExerciseSlope',\n",
        "                     'MajorVessels', 'ThalliumStress', 'MaxHeartRate_binned', 'OldPeak_binned']\n",
        "X = df[relevant_features]\n",
        "# If you want them all:\n",
        "# X = df.drop('HeartDisease' , axis=1)\n",
        "y = df['HeartDisease']\n",
        "\n",
        "# Scaling the data improves stability, consistency and performance.\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Same random number generated every time: Reproducibility\n",
        "np.random.seed(564)\n",
        "test_sizes = {}\n",
        "\n",
        "#for i in range(5, 51, 5):\n",
        "# Commonly 80/20, 70/30, 50/50.\n",
        "for i in [20, 30, 50]:\n",
        "    test_sizes[f\"{i}%\"] = i / 100\n",
        "\n",
        "count = 0\n",
        "best_models = {}\n",
        "for sizeOfT, testSize in test_sizes.items():\n",
        "    print(f\"\\n---Iteration {count} Test size: {sizeOfT}:\")\n",
        "    count += 1\n",
        "\n",
        "    # Split data using the fixed random state\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize, random_state=200) # Same split of data everytime\n",
        "\n",
        "    # All models used\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='sag'),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(),\n",
        "        \"SVM\": SVC()\n",
        "    }\n",
        "\n",
        "    modelPerformance = {}\n",
        "\n",
        "    # Initialize StratifiedKFold\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=200)\n",
        "\n",
        "    # Fit the models\n",
        "    for model_name, model in models.items():\n",
        "        # Normal method: Train and evaluate the model on the test set\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        test_accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{model_name} with Test Accuracy: {test_accuracy:.3f}\")\n",
        "        modelPerformance[model_name] = (avg_cv_score, test_accuracy)\n",
        "\n",
        "        # StratifiedKFold for cross-validation\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=skf)\n",
        "        avg_cv_score = cv_scores.mean()\n",
        "        print(f\"{model_name} with Cross-Validation Accuracy: {avg_cv_score:.3f}\")\n",
        "\n",
        "    # Find and print the best model for this test size based on cross-validation score\n",
        "    best_model_name = max(modelPerformance, key=lambda x: modelPerformance[x])\n",
        "    best_model_cv_accuracy, best_model_test_accuracy = modelPerformance[best_model_name]\n",
        "    print(f\"Best Model: {best_model_name} with Cross-Validation Accuracy: {best_model_cv_accuracy:.3f} and Test Accuracy: {best_model_test_accuracy:.3f}\")\n",
        "\n",
        "    # Store the best model for each test size\n",
        "    best_models[sizeOfT] = (best_model_name, best_model_cv_accuracy, best_model_test_accuracy)"
      ],
      "metadata": {
        "id": "qqcfCanNkdzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display best models for each test size\n",
        "print(\"\\nBest Models for Each Test Size:\")\n",
        "for test_size, (best_model, cv_accuracy, test_accuracy) in best_models.items():\n",
        "    print(f\"Test Size: {test_size}\\n  Best Model: {best_model}\\n  CV Accuracy: {cv_accuracy:.3f}\\n  Test Accuracy: {test_accuracy:.3f}\\n\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "test_sizes = list(best_models.keys())\n",
        "models = [\"Logistic Regression\", \"Decision Tree\", \"SVM\"]\n",
        "\n",
        "for test_size in test_sizes:\n",
        "    cv_accuracies = [modelPerformance[model][0] for model in models]\n",
        "    test_accuracies = [modelPerformance[model][1] for model in models]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    x = np.arange(len(models))\n",
        "    bar_width = 0.35\n",
        "\n",
        "    # Plot cross-validation and test accuracy bars\n",
        "    ax.bar(x - bar_width/2, cv_accuracies, bar_width, label='CV Accuracy')\n",
        "    ax.bar(x + bar_width/2, test_accuracies, bar_width, label='Test Accuracy')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(models)\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_title(f'Model Performance at {test_size}')\n",
        "    ax.legend(loc='lower left')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1UrHiUWSAiwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}