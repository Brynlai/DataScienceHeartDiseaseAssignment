{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brynlai/DataScienceHeartDiseaseAssignment/blob/Bryan/HeartDiseasev1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rQJYLjk7o-mg"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install ucimlrepo\n",
        "!pip install pandas matplotlib seaborn scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Fetch the Heart Disease dataset from the UCI ML Repository\n",
        "heart_disease_bunch = fetch_ucirepo(id=45)\n",
        "\n",
        "# Print the fetched dataset\n",
        "print(heart_disease_bunch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC6_zKXU8MPa"
      },
      "outputs": [],
      "source": [
        "# Load into DataFrame\n",
        "heart_disease = pd.DataFrame(data=heart_disease_bunch.data.features,\n",
        "                             columns=heart_disease_bunch.data.feature_names,\n",
        "                             index=heart_disease_bunch.data.ids)\n",
        "\n",
        "heart_disease = pd.concat([heart_disease, heart_disease_bunch.data.targets], axis=1)\n",
        "df = heart_disease\n",
        "print(df.info())\n",
        "\n",
        "df = df.rename(columns={'num': 'target'})\n",
        "df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)\n",
        "column_names = {\n",
        "    \"age\": \"Age\",\n",
        "    \"sex\": \"Gender\",\n",
        "    \"cp\": \"ChestPainType\",\n",
        "    \"trestbps\": \"RestingBP\",\n",
        "    \"fbs\": \"FastBloodSugar\",\n",
        "    \"restecg\": \"RestingECG\",\n",
        "    \"exang\": \"ExerciseAngina\",\n",
        "    \"slope\": \"ExerciseSlope\",\n",
        "    \"ca\": \"MajorVessels\",\n",
        "    \"thal\": \"ThalliumStress\",\n",
        "    \"target\": \"HeartDisease\",\n",
        "    \"chol\": \"SerumCholesterol\",\n",
        "    \"thalach\": \"MaxHeartRate\",\n",
        "    \"oldpeak\": \"OldPeak\"\n",
        "}\n",
        "df.rename(columns=column_names, inplace=True)\n",
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtzvYm_rSZOB"
      },
      "outputs": [],
      "source": [
        "# DATA CLEANING : Dealing with duplicate observation\n",
        "# Check for any duplicate observation\n",
        "duplicate_rows = df.duplicated()\n",
        "print(\"Number of duplicate rows before:\", duplicate_rows.sum())\n",
        "\n",
        "# Remove duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Check for duplicate rows again\n",
        "duplicate_rows = df.duplicated()\n",
        "print(\"Number of duplicate rows after:\", duplicate_rows.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTxsFBhESeN5"
      },
      "outputs": [],
      "source": [
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Replace missing values with the median of each column, only if the column is numerical\n",
        "for column in df.columns:\n",
        "    if df[column].dtype in [np.int64, np.float64]:  # Check if the column is numerical\n",
        "        df[column] = df[column].fillna(df[column].median())\n",
        "\n",
        "# Check if there are any missing values left\n",
        "missing_values_after = df.isnull().sum()\n",
        "print(\"Missing values after replacing with medians:\")\n",
        "print(missing_values_after)\n",
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmVRYQtBXvz2"
      },
      "source": [
        "Remove Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMfAGVvdXvk1"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(df, columns, threshold=4):\n",
        "    outliers = {}\n",
        "    for column in columns:\n",
        "        mean = df[column].mean()\n",
        "        std = df[column].std()\n",
        "        z_scores = np.abs((df[column] - mean) / std)\n",
        "        outlier_indices = z_scores >= threshold\n",
        "        outliers[column] = df[column][outlier_indices].tolist()\n",
        "        df = df[~outlier_indices]\n",
        "    return df, outliers\n",
        "\n",
        "# Specify the columns to check for outliers\n",
        "columns_to_check = ['RestingBP', 'SerumCholesterol', 'MaxHeartRate', 'OldPeak']\n",
        "\n",
        "# Remove outliers from the DataFrame and print the outliers\n",
        "df_cleaned, outliers = remove_outliers(df, columns_to_check)\n",
        "for column, outlier_list in outliers.items():\n",
        "    print(f\"Outliers in {column}: {outlier_list}\")\n",
        "\n",
        "# Create distribution plots before and after removing outliers\n",
        "fig, axes = plt.subplots(nrows=len(columns_to_check), ncols=2, figsize=(12, 6*len(columns_to_check)))\n",
        "for i, column in enumerate(columns_to_check):\n",
        "    sns.histplot(df[column], ax=axes[i, 0], kde=True, bins=20)\n",
        "    axes[i, 0].set_title(f'Distribution of {column} Before Removing Outliers')\n",
        "    sns.histplot(df_cleaned[column], ax=axes[i, 1], kde=True, bins=20)\n",
        "    axes[i, 1].set_title(f'Distribution of {column} After Removing Outliers')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln2rbENaS7Eg"
      },
      "outputs": [],
      "source": [
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk5GPMa93feu"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax = sns.heatmap(corr_matrix,\n",
        "                 annot=True,\n",
        "                 linewidths=0.5,\n",
        "                 fmt=\".2f\",\n",
        "                 cmap=\"YlGnBu\");\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqcfCanNkdzm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare the data\n",
        "X = df.drop('HeartDisease', axis=1)\n",
        "y = df['HeartDisease']\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "np.random.seed(564)\n",
        "test_sizes = {}\n",
        "for i in range(15, 23):\n",
        "    test_sizes[f\"{i}%\"] = i / 100\n",
        "\n",
        "count = 0\n",
        "best_models = {}\n",
        "for sizeOfT, testSize in test_sizes.items():\n",
        "    print(f\"\\n---Iteration {count} Test size: {sizeOfT}:\")\n",
        "    count += 1\n",
        "\n",
        "    # Split the data using the fixed random state\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize, random_state=200)\n",
        "\n",
        "    # Define the models\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='sag'),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(),\n",
        "        \"Random Forest\": RandomForestClassifier(),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "        \"SVM\": SVC()\n",
        "    }\n",
        "\n",
        "    modelPerformance = {}\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    for model_name, model in models.items():\n",
        "        # Use 5-fold cross-validation as an example\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "        avg_cv_score = cv_scores.mean()\n",
        "        print(f\"{model_name} with Cross-Validation Accuracy: {avg_cv_score:.3f}\")\n",
        "\n",
        "        # Train and evaluate the model on the test set\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        test_accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"{model_name} with Test Accuracy: {test_accuracy:.3f}\")\n",
        "        modelPerformance[model_name] = (avg_cv_score, test_accuracy)\n",
        "\n",
        "    # Find and print the best model for this test size based on cross-validation score\n",
        "    best_model_name = max(modelPerformance, key=lambda x: modelPerformance[x])\n",
        "    best_model_cv_accuracy, best_model_test_accuracy = modelPerformance[best_model_name]\n",
        "    print(f\"Best Model: {best_model_name} with Cross-Validation Accuracy: {best_model_cv_accuracy:.3f} and Test Accuracy: {best_model_test_accuracy:.3f}\")\n",
        "\n",
        "    # Store the best model for each test size\n",
        "    best_models[sizeOfT] = (best_model_name, best_model_cv_accuracy, best_model_test_accuracy)\n",
        "\n",
        "# Print the best models for each test size\n",
        "print(\"\\nBest Models for Each Test Size:\")\n",
        "for test_size, (best_model, cv_accuracy, test_accuracy) in best_models.items():\n",
        "    print(f\"Test Size: {test_size}, Best Model: {best_model} with Cross-Validation Accuracy: {cv_accuracy:.3f} and Test Accuracy: {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the required module from scipy\n",
        "from scipy import stats\n",
        "\n",
        "# Separate the data into two groups based on the presence of heart disease\n",
        "age_with_disease = df[df['HeartDisease'] == 1]['Age']\n",
        "age_without_disease = df[df['HeartDisease'] == 0]['Age']\n",
        "\n",
        "# Perform an independent samples t-test\n",
        "t_statistic, p_value = stats.ttest_ind(age_with_disease, age_without_disease)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Output the results\n",
        "print(f'T-statistic: {t_statistic}')\n",
        "print(f'P-value: {p_value}')\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(f\"Since {p_value} < {alpha}, we reject the null hypothesis (H₀) and conclude that there is a significant difference in mean age between patients with and without heart disease.\")\n",
        "else:\n",
        "    print(f\"Since {p_value} >= {alpha}, we do not reject the null hypothesis (H₀) and conclude that there is no significant difference in mean age between patients with and without heart disease.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUmWetlKBt_A"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DymvVZ1BvTw"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
