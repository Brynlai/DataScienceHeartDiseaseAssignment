{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brynlai/DataScienceHeartDiseaseAssignment/blob/Bryan/StratifiedKFold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install ucimlrepo\n",
        "!pip install pandas matplotlib seaborn scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Fetch the Heart Disease dataset from the UCI ML Repository\n",
        "heart_disease_bunch = fetch_ucirepo(id=45)\n",
        "\n",
        "# Print the fetched dataset\n",
        "print(heart_disease_bunch)"
      ],
      "metadata": {
        "id": "rQJYLjk7o-mg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load into DataFrame\n",
        "heart_disease = pd.DataFrame(data=heart_disease_bunch.data.features,\n",
        "                             columns=heart_disease_bunch.data.feature_names,\n",
        "                             index=heart_disease_bunch.data.ids)\n",
        "\n",
        "heart_disease = pd.concat([heart_disease, heart_disease_bunch.data.targets], axis=1)\n",
        "df = heart_disease\n",
        "print(df.info())\n",
        "\n",
        "df = df.rename(columns={'num': 'target'})\n",
        "df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "# Iterate over each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability"
      ],
      "metadata": {
        "id": "gC6_zKXU8MPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = {\n",
        "    \"age\": \"Age\",\n",
        "    \"sex\": \"Gender\",\n",
        "    \"cp\": \"ChestPainType\",\n",
        "    \"trestbps\": \"RestingBP\",\n",
        "    \"fbs\": \"FastBloodSugar\",\n",
        "    \"restecg\": \"RestingECG\",\n",
        "    \"exang\": \"ExerciseAngina\",\n",
        "    \"slope\": \"ExerciseSlope\",\n",
        "    \"ca\": \"MajorVessels\",\n",
        "    \"thal\": \"ThalliumStress\",\n",
        "    \"target\": \"HeartDisease\",\n",
        "    \"chol\": \"SerumCholesterol\",\n",
        "    \"thalach\": \"MaxHeartRate\",\n",
        "    \"oldpeak\": \"OldPeak\"\n",
        "}\n",
        "df.rename(columns=column_names, inplace=True)\n",
        "# print(\"------- Renamed df:\", df)\n",
        "\n",
        "# Export the entire DataFrame to a CSV file (including all rows and columns)\n",
        "# heart_disease.to_csv('heart_disease_full.csv', index=False)\n",
        "# Iterate over each column in the DataFrame\n"
      ],
      "metadata": {
        "id": "DzHjMHGZ7tMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA CLEANING : Dealing with duplicate observation\n",
        "# Check for any duplicate observation\n",
        "duplicate_rows = df.duplicated()\n",
        "print(\"Number of duplicate rows before:\", duplicate_rows.sum())\n",
        "\n",
        "# Remove duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Check for duplicate rows again\n",
        "duplicate_rows = df.duplicated()\n",
        "print(\"Number of duplicate rows after:\", duplicate_rows.sum())\n"
      ],
      "metadata": {
        "id": "AtzvYm_rSZOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Replace missing values with the median of each column, only if the column is numerical\n",
        "for column in df.columns:\n",
        "    if df[column].dtype in [np.int64, np.float64]:  # Check if the column is numerical\n",
        "        df[column] = df[column].fillna(df[column].median())\n",
        "\n",
        "# Check if there are any missing values left\n",
        "missing_values_after = df.isnull().sum()\n",
        "print(\"Missing values after replacing with medians:\")\n",
        "print(missing_values_after)\n",
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability"
      ],
      "metadata": {
        "id": "rTxsFBhESeN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(df, columns, threshold=3):\n",
        "    outliers = {}\n",
        "    for column in columns:\n",
        "        mean = df[column].mean()\n",
        "        std = df[column].std()\n",
        "        z_scores = np.abs((df[column] - mean) / std)\n",
        "        outlier_indices = z_scores >= threshold\n",
        "        outliers[column] = df[column][outlier_indices].tolist()\n",
        "        df = df[~outlier_indices]\n",
        "    return df, outliers\n",
        "\n",
        "# Specify the columns to check for outliers\n",
        "columns_to_check = ['RestingBP', 'SerumCholesterol', 'MaxHeartRate', 'OldPeak']\n",
        "\n",
        "# Remove outliers from the DataFrame and print the outliers\n",
        "df_cleaned, outliers = remove_outliers(df, columns_to_check)\n",
        "for column, outlier_list in outliers.items():\n",
        "    print(f\"Outliers in {column}: {outlier_list}\")\n",
        "\n",
        "# Create distribution plots before and after removing outliers\n",
        "fig, axes = plt.subplots(nrows=len(columns_to_check), ncols=2, figsize=(12, 6*len(columns_to_check)))\n",
        "for i, column in enumerate(columns_to_check):\n",
        "    sns.histplot(df[column], ax=axes[i, 0], kde=True, bins=20)\n",
        "    axes[i, 0].set_title(f'Distribution of {column} Before Removing Outliers')\n",
        "    sns.histplot(df_cleaned[column], ax=axes[i, 1], kde=True, bins=20)\n",
        "    axes[i, 1].set_title(f'Distribution of {column} After Removing Outliers')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability"
      ],
      "metadata": {
        "id": "MMqU1E4tSjMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define bins for age\n",
        "age_bins = [10, 20, 30, 40, 50, 60, np.inf]\n",
        "age_labels = [1, 2, 3, 4, 5, 6]  # Assign numerical labels\n",
        "df['Age_binned'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, include_lowest=True)\n",
        "\n",
        "# Define bins for resting blood pressure\n",
        "trestbps_bins = [0, 100, 120, 140, 160, np.inf]\n",
        "trestbps_labels = [1, 2, 3, 4, 5]  # Assign numerical labels\n",
        "df['RestingBP_binned'] = pd.cut(df['RestingBP'], bins=trestbps_bins, labels=trestbps_labels, include_lowest=True)\n",
        "\n",
        "# Define bins for serum cholesterol\n",
        "chol_bins = [0, 160, 200, 240, 280, np.inf]\n",
        "chol_labels = [1, 2, 3, 4, 5]  # Assign numerical labels\n",
        "df['SerumCholesterol_binned'] = pd.cut(df['SerumCholesterol'], bins=chol_bins, labels=chol_labels, include_lowest=True)\n",
        "\n",
        "# Define bins for maximum heart rate achieved\n",
        "thalach_bins = [0, 90, 120, 150, 180, np.inf]\n",
        "thalach_labels = [1, 2, 3, 4, 5]  # Assign numerical labels\n",
        "df['MaxHeartRate_binned'] = pd.cut(df['MaxHeartRate'], bins=thalach_bins, labels=thalach_labels, include_lowest=True)\n",
        "\n",
        "# Define bins for ST depression induced by exercise\n",
        "oldpeak_bins = [0, 0.5, 1.5, 2.5, 3.5, np.inf]\n",
        "oldpeak_labels = [1, 2, 3, 4, 5]  # Assign numerical labels\n",
        "df['OldPeak_binned'] = pd.cut(df['OldPeak'], bins=oldpeak_bins, labels=oldpeak_labels, include_lowest=True)\n",
        "\n",
        "# Drop original numerical columns if desired\n",
        "df = df.drop(['Age', 'RestingBP', 'SerumCholesterol', 'MaxHeartRate', 'OldPeak'], axis=1)\n",
        "\n",
        "for column in df.columns:\n",
        "    # Group by the column and get the size of each group\n",
        "    group_sizes = df.groupby(column).size()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(group_sizes)\n",
        "    print()  # Empty line for better readability"
      ],
      "metadata": {
        "id": "lmpOp7ws7H0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax = sns.heatmap(corr_matrix,\n",
        "                 annot=True,\n",
        "                 linewidths=0.5,\n",
        "                 fmt=\".2f\",\n",
        "                 cmap=\"YlGnBu\");\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fk5GPMa93feu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare the data\n",
        "X = df.drop('HeartDisease' , axis=1)\n",
        "y = df['HeartDisease']\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "np.random.seed(564)\n",
        "test_sizes = {}\n",
        "for i in range(5, 30):\n",
        "    test_sizes[f\"{i}%\"] = i / 100\n",
        "\n",
        "count = 0\n",
        "best_models = {}\n",
        "for sizeOfT, testSize in test_sizes.items():\n",
        "    print(f\"\\n---Iteration {count} Test size: {sizeOfT}:\")\n",
        "    count += 1\n",
        "\n",
        "    # Split the data using the fixed random state\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize, random_state=200)\n",
        "\n",
        "    # Define the models\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='sag'),\n",
        "        # \"Decision Tree\": DecisionTreeClassifier(),\n",
        "        # \"Random Forest\": RandomForestClassifier(),\n",
        "        # \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "        \"SVM\": SVC()\n",
        "    }\n",
        "\n",
        "    modelPerformance = {}\n",
        "\n",
        "    # Initialize StratifiedKFold\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=200)\n",
        "\n",
        "    # Perform stratified k-fold cross-validation\n",
        "    for model_name, model in models.items():\n",
        "        # Use StratifiedKFold for cross-validation\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=skf)\n",
        "        avg_cv_score = cv_scores.mean()\n",
        "        print(f\"{model_name} with Cross-Validation Accuracy: {avg_cv_score:.3f}\")\n",
        "\n",
        "        # # Train and evaluate the model on the test set\n",
        "        # model.fit(X_train, y_train)\n",
        "        # y_pred = model.predict(X_test)\n",
        "        # test_accuracy = accuracy_score(y_test, y_pred)\n",
        "        # print(f\"{model_name} with Test Accuracy: {test_accuracy:.3f}\")\n",
        "        modelPerformance[model_name] = (avg_cv_score, None)\n",
        "\n",
        "    # Find and print the best model for this test size based on cross-validation score\n",
        "    best_model_name = max(modelPerformance, key=lambda x: modelPerformance[x])\n",
        "    best_model_cv_accuracy, _ = modelPerformance[best_model_name]\n",
        "    print(f\"Best Model: {best_model_name} with Cross-Validation Accuracy: {best_model_cv_accuracy:.3f}\")\n",
        "\n",
        "    # Store the best model for each test size\n",
        "    best_models[sizeOfT] = (best_model_name, best_model_cv_accuracy, None)\n",
        "\n",
        "# Print the best models for each test size\n",
        "print(\"\\nBest Models for Each Test Size:\")\n",
        "for test_size, (best_model, cv_accuracy, _) in best_models.items():\n",
        "    # print(f\" Test Size: {test_size}, Best Model:\\n {best_model} with Cross-Validation Accuracy: {cv_accuracy:.3f} and (Ignore) Test Accuracy: {test_accuracy:.3f}\")\n",
        "    print(f\" Test Size: {test_size}, Best Model:\\n {best_model} with Cross-Validation Accuracy: {cv_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "qqcfCanNkdzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
